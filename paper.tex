\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Creating a Neural Network for MNIST Classification Using NumPy}
\author{Badger Code}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper details the implementation of a neural network from scratch using only NumPy to classify handwritten digits from the MNIST dataset. We cover the forward pass using ReLU and softmax functions, the computation of the categorical cross-entropy loss, and backpropagation for gradient descent optimization.
\end{abstract}

\section{Introduction}
Neural networks have become a fundamental tool in the field of machine learning, particularly for tasks involving image recognition. In this paper, we demonstrate how to implement a simple neural network using only NumPy to classify digits from the MNIST dataset. This implementation includes the forward pass, loss computation, and backpropagation.

\section{Data Preparation}
We use the MNIST dataset, which contains 70,000 images of handwritten digits (0-9). Each image is 28x28 pixels. The dataset is normalized and split into training and test sets. Labels are one-hot encoded.

\section{Neural Network Architecture}
Our neural network consists of an input layer with 784 neurons (28x28 pixels), a hidden layer with 128 neurons, and an output layer with 10 neurons (one for each digit).

\subsection{Initialization}
Weights and biases are initialized as follows:
\begin{align}
\mathbf{W}_1 &\sim \mathcal{N}(0, 0.01), & \mathbf{b}_1 &= \mathbf{0} \\
\mathbf{W}_2 &\sim \mathcal{N}(0, 0.01), & \mathbf{b}_2 &= \mathbf{0}
\end{align}

\section{Forward Pass}
The forward pass consists of computing the activations for each layer using the ReLU activation function for the hidden layer and the softmax function for the output layer.

\subsection{ReLU Activation Function}
The ReLU activation function is defined as:
\begin{equation}
\text{ReLU}(z) = \max(0, z)
\end{equation}

\subsection{Softmax Function}
The softmax function is used to compute the probabilities of each class:
\begin{equation}
\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\end{equation}

\subsection{Forward Propagation Equations}
The forward propagation through the network is given by:
\begin{align}
\mathbf{Z}_1 &= \mathbf{X} \mathbf{W}_1 + \mathbf{b}_1 \\
\mathbf{A}_1 &= \text{ReLU}(\mathbf{Z}_1) \\
\mathbf{Z}_2 &= \mathbf{A}_1 \mathbf{W}_2 + \mathbf{b}_2 \\
\mathbf{A}_2 &= \text{softmax}(\mathbf{Z}_2)
\end{align}

\section{Loss Function}
The categorical cross-entropy loss function measures the performance of the classification model:
\begin{equation}
L(\mathbf{y}, \hat{\mathbf{y}}) = -\frac{1}{m} \sum_{i=1}^m \sum_{j=1}^K y_{ij} \log(\hat{y}_{ij})
\end{equation}
where \( \mathbf{y} \) is the true label, \( \hat{\mathbf{y}} \) is the predicted probability, \( m \) is the number of samples, and \( K \) is the number of classes.

\section{Backpropagation}
Backpropagation is used to update the weights and biases by computing the gradients of the loss function with respect to the parameters.

\subsection{Gradient Computation}
The gradients are computed as follows:
\begin{align}
\mathbf{dZ}_2 &= \mathbf{A}_2 - \mathbf{Y} \\
\mathbf{dW}_2 &= \frac{1}{m} \mathbf{A}_1^\top \mathbf{dZ}_2 \\
\mathbf{db}_2 &= \frac{1}{m} \sum_{i=1}^m \mathbf{dZ}_2 \\
\mathbf{dA}_1 &= \mathbf{dZ}_2 \mathbf{W}_2^\top \\
\mathbf{dZ}_1 &= \mathbf{dA}_1 \odot \text{ReLU}'(\mathbf{Z}_1) \\
\mathbf{dW}_1 &= \frac{1}{m} \mathbf{X}^\top \mathbf{dZ}_1 \\
\mathbf{db}_1 &= \frac{1}{m} \sum_{i=1}^m \mathbf{dZ}_1
\end{align}

\subsection{Parameter Update}
The parameters are updated using gradient descent:
\begin{align}
\mathbf{W}_1 &\leftarrow \mathbf{W}_1 - \alpha \mathbf{dW}_1 \\
\mathbf{b}_1 &\leftarrow \mathbf{b}_1 - \alpha \mathbf{db}_1 \\
\mathbf{W}_2 &\leftarrow \mathbf{W}_2 - \alpha \mathbf{dW}_2 \\
\mathbf{b}_2 &\leftarrow \mathbf{b}_2 - \alpha \mathbf{db}_2
\end{align}
where \( \alpha \) is the learning rate.

\section{Training the Neural Network}
We train the neural network on the training data using a specified number of epochs and learning rate. The loss is printed every 100 epochs to monitor the training process.

\section{Evaluation}
After training, we evaluate the model on the test set and compute the accuracy:
\begin{equation}
\text{accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}} \times 100
\end{equation}

\section{Visualization}
We visualize some predictions to see how well the model performs on individual samples.

\section{Conclusion}
This paper demonstrated the implementation of a neural network using only NumPy to classify MNIST digits. The network was trained using gradient descent and evaluated on the test set to measure its accuracy.

\end{document}